---
title: "Employee Turnover Prediction"
author: "Albesa Istrefaj, Antonia Durisch, Philipp Drebes"
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: yes
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to our project on employee turnover prediction! In this project, we will be using various machine learning algorithms to analyze a dataset containing employee turnover data and develop predictive models.

Employee turnover is a critical issue for companies, as it can result in significant costs such as recruitment, training, and lost productivity. Identifying factors that contribute to employee turnover and developing accurate predictive models can help companies better understand employee behavior and improve employee retention.

## Data set

The dataset we will be using is the [Employee Turnover](https://www.kaggle.com/datasets/davinwijaya/employee-turnover) data set. This data set contains the following information about the employees of a company.

| Column       | Description                                                                                                                                                |
|-------------------|-----------------------------------------------------|
| stag         | Experience (time)                                                                                                                                          |
| event        | Employee turnover                                                                                                                                          |
| gender       | Employee's gender, female(f), or male(m)                                                                                                                   |
| age          | Employee's age (year)                                                                                                                                      |
| industry     | Employee's Industry                                                                                                                                        |
| profession   | Employee's profession                                                                                                                                      |
| traffic      | From what pipelene candidate came to the company                                                                                                           |
| coach        | Presence of a coach (training) on probation                                                                                                                |
| head_gender  | head (supervisor) gender                                                                                                                                   |
| greywage     | Greywage in Russia or Ukraine means that the employer (company) pays just a tiny bit amount of salary above the white-wage (white-wage means minimum wage) |
| way          | how an employee gets to workplace (by feet, by bus etc)                                                                                                    |
| extraversion | result from Big Five personality test                                                                                                                      |
| independ     | result from Big Five personality test                                                                                                                      |
| selfcontrol  | result from Big Five personality test                                                                                                                      |
| anxiety      | result from Big Five personality test                                                                                                                      |
| novator      | result from Big Five personality test                                                                                                                      |

## Preparation and Exploration

Load the libraries.

```{r, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse, quietly = T)
library(vioplot, quietly = T)
library(caret, quietly = T)
library(vtreat, quietly = T)
library(ggcorrplot, quietly = T)
library(readr, quietly = T)
library(ggplot2, quietly = T)
library(gridExtra, quietly = T)
library(mgcv, quietly = T)
library(e1071, quietly = T)
library(dplyr, quietly = T)
library(mlbench, quietly = T)
library(caret, quietly = T)
library(arm, quietly = T)
library(doParallel, quietly = T)
```

Load and inspect the data.

```{r}
turnover <- read_csv('data/turnover.csv', col_names=TRUE)
# summary(turnover)

# Having a look at the structure.
str(turnover)
```

## Exploration

```{r}
# scatterplot of stag vs. extraversion
plot.scatter <- ggplot(data = turnover, aes(x = stag, y = extraversion)) +
  geom_point()

# visualizing the distribution of a categorical variable - distribution of industry
plot.distca <- ggplot(data = turnover, aes(x = industry)) +
  geom_bar()

# distribution of a continuous variable - box plot of age by gender
plot.distco <- ggplot(data = turnover, aes(x = gender, y = age)) +
  geom_boxplot()

grid.arrange(plot.scatter, plot.distca, plot.distco, ncol=2)

par(mfrow = c(2,3))

# histogram of the age
hist(turnover$age)

# boxplot of the age
boxplot(turnover$age)

# plot of age and extraversion
plot(turnover$age, turnover$extraversion)

# barplot of gender - male and female
barplot(table(turnover$gender))

# density plots
plot(density(turnover$age))

# violin plot
vioplot(turnover$age)
```

### One-Hot encoding

```{r}
dummy <- dummyVars(" ~ .", data=turnover)

#perform one-hot encoding on data frame
turnover.oh <- data.frame(predict(dummy, newdata=turnover))

str(turnover.oh)
```

We will normalize all scalar values.

```{r}
# Normalize selected columns
columns_to_normalize <- c("extraversion", "independ", "selfcontrol", "anxiety", "novator", "age", "stag")
turnover.oh[columns_to_normalize] <- scale(turnover.oh[columns_to_normalize], center = FALSE, scale = apply(turnover.oh[columns_to_normalize], 2, range)[2, ] - apply(turnover.oh[columns_to_normalize], 2, range)[1, ])
```

Next we remove highly correlated variables from our data set, as they might have a negative influence on our models.

```{r}
correlationMatrix <- cor(turnover.oh)
ggcorrplot(correlationMatrix, hc.order = TRUE)

# find attributes that are highly corrected ( > 0.75 )
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated <- sort(highlyCorrelated)

# print indexes of highly correlated attributes
print(highlyCorrelated)
names(turnover.oh[highlyCorrelated])

turnover.reduced = turnover.oh[,-c(highlyCorrelated)]

ggcorrplot(cor(turnover.reduced), hc.order = TRUE)
```

The data types of some variables need to be modified.

```{r}
turnover.reduced$event = as.factor(turnover.reduced$event) # Convert event as factor
turnover.reduced$age <- as.integer(turnover.reduced$age) # Make sure all ages are integers
```

## Linear Models

In this chapter, we will be exploring linear models and try to use them to predict the likelihood of an employee leaving the company.

Our primary research topic is employee turnover, which is recorded by the binomial variable `event` in our data set. Later we will analyze this dependent variable with the use of GLMs, GAMs, SVM and ANN.

First let's try if we can predict the experience (time) of an employee, given their industry, profession, age and gender. As we expect them to have an effect.

```{r}
fit <- lm(stag ~ industry + profession + age + gender, data = turnover)
summary(fit)
```

We see that we have multiple significant factors in our model. However, only around 11% of the variance is explained by the model. It might not be as useful for further predictions and we would need to gather more data.

### Non-linearities

First we inspect all numerical coefficients for non-linearities.

```{r}
## Age
gg.age <- ggplot(data = turnover, mapping = aes(y = event, x = age)) + 
  geom_point()

plot.age <- gg.age + geom_smooth()

## Extraversion
gg.extraversion <- ggplot(data = turnover, mapping = aes(y = event, x = extraversion)) + 
  geom_point()

plot.extraversion <- gg.extraversion +  geom_smooth()

## Independence
gg.independence <- ggplot(data = turnover, mapping = aes(y = event, x = independ)) + 
  geom_point()

plot.independence <- gg.independence + geom_smooth()

## Self-control
gg.selfcontrol <- ggplot(data = turnover, mapping = aes(y = event, x = selfcontrol)) + 
  geom_point()

plot.selfcontrol <- gg.selfcontrol + geom_smooth()

## Anxiety
gg.anxiety <- ggplot(data = turnover, mapping = aes(y = event, x = anxiety)) + 
  geom_point()

plot.anxiety <- gg.anxiety + geom_smooth()

## Innovator
gg.innovator <- ggplot(data = turnover, mapping = aes(y = event, x = novator)) + 
  geom_point()

plot.innovator <- gg.innovator + geom_smooth()

grid.arrange(plot.age, plot.extraversion, gg.independence, plot.selfcontrol, plot.anxiety, plot.innovator, ncol=2)
```

It appears that only the factor age has a non-linear behavior. We will use a GAM to find its complexity.

#### Training the model

Performing feature selection before training a model can be a good practice to improve model performance and reduce overfitting. Here we use recursive feature elimination (RFE) before training the model. We will again use only the one-hot encoded data set with correlating features included, as we expect the feature selection process to take care of it.

```{r}
set.seed(12)

# Define the control parameters for feature selection
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

# Specify the formula for the GAM model
formula <- event ~ s(age, bs = "tp", k = -1) + .

# Define a smaller range of subset sizes
subset_sizes <- c(1:10)

# Reduce the number of values in the tuning grid
tune_grid <- data.frame(nselect = 1:10)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# Run the feature selection algorithm
rfe_result <- rfe(turnover.oh[, -2], turnover.oh$event,
                   sizes = subset_sizes, rfeControl = control,
                   method = "gam", tuneGrid = tune_grid, verbose = FALSE)

# Stop the parallel processing
stopCluster(cl)

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

unregister_dopar()

# Get the selected features
selected_features <- names(turnover.reduced[, -2])[rfe_result$optVariables]
print(selected_features)
```

No features where selected. Therefore, we will use again the `turnover.reduced` data set. We will try a different approach.

```{r}
# ensure results are repeatable
set.seed(12)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# train the model
model.gam <- train(event ~ ., data=turnover.reduced, family = "binomial", 
               method="gam", preProcess=c("scale", "center"), trControl=control)

# Stop the parallel processing
stopCluster(cl)
unregister_dopar()

# estimate variable importance
importance <- varImp(model.gam, scale=FALSE)

# summarize importance
# print(importance)
plot(importance)

summary(model.gam)
plot(model.gam, residuals = TRUE)
AIC(model.gam$finalModel)
```

Still not a good fit, with only around 12% of the variance explained by the model. Also we see, that the feature selection only has a slight, almost negligible, impact on accuracy.

### Generalized Linear Models

Next we fit a generalized linear model in order to predict employee turnover.

```{r}
# ensure results are repeatable
set.seed(12)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# train the model
model.glm <- train(event ~ ., data=turnover.reduced, family = "binomial", 
               method="glm", preProcess=c("scale", "center"), trControl=control)

# Stop the parallel processing
stopCluster(cl)
unregister_dopar()

# estimate variable importance
importance <- varImp(model.glm, scale=FALSE)

# summarize importance
# print(importance)
plot(importance)

summary(model.glm)
```

We also want to try to predict an employees age by their amount of experience and multiple other factors.

```{r}
glm.age <- glm(age ~ ., data = turnover.reduced, family = "poisson")
summary(glm.age)


set.seed(2)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# train the model
model.glm <- train(age ~ ., data=turnover.reduced, family = "poisson", 
               method="glm", preProcess=c("scale", "center"), trControl=control)

# Stop the parallel processing
stopCluster(cl)
unregister_dopar()

# estimate variable importance
importance <- varImp(model.glm, scale=FALSE)

# summarize importance
# print(importance)
plot(importance)

summary(model.glm)
```

## Support Vector Machines

In this chapter, we will be use Support Vector Machines (SVM) in order to predict employee turnover.

```{r}
set.seed(123)
indices <- createDataPartition(turnover.reduced$event, p=.85, list = F)

train <- turnover.reduced %>%
  slice(indices[, 1])
test_in <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  dplyr::select(-event)
test_truth <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  pull(event)
```

### Train the Neural Network

```{r}
set.seed(100)
# Fit the SVM model on the training data
#svm.linear <- svm(event ~ ., data = train, kernel = "linear",
#                 type = "C-classification", scale = TRUE, cost = 10)

tune.out <- tune(
  svm, event ~ ., data = train, kernel = "linear", type = "C-classification", scale = TRUE,
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
svm.linear <- tune.out$best.model

# Make predictions on the testing data
test_pred <- predict(svm.linear, newdata = test_in)

# Convert test_truth to a factor with the same levels as test_pred
test_truth <- factor(test_truth, levels = levels(test_pred))

# Evaluate the performance of the model using confusion matrix
conf_matrix <- confusionMatrix(test_pred, test_truth)
conf_matrix
```

Now we will use the `tune` function again and also try different kernels for the SVM algorithm.

```{r}
set.seed(99)
tune.out <- tune(
  svm, event ~ ., data = train, type = "C-classification", scale = TRUE,
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                kernel = c("linear", "radial", "polynomial", "sigmoid"))
)
print(tune.out)

svm.best <- tune.out$best.model

# Make predictions on the testing data
test_pred <- predict(svm.best, newdata = test_in)

# Convert test_truth to a factor with the same levels as test_pred
test_truth <- factor(test_truth, levels = levels(test_pred))

# Evaluate the performance of the model using confusion matrix
conf_matrix <- confusionMatrix(test_pred, test_truth)
conf_matrix
```

### Conclusion

## Artificial Neural Networks

In this chapter, we will be exploring Artificial Neural Networks (ANN). ANN is a machine learning algorithm inspired by the biological structure of the human brain. We will explain how ANN works, how to train an ANN model, and how to use it to predict employee turnover.

```{r}

```
