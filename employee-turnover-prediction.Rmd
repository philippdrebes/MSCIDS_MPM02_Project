---
title: "Employee Turnover Prediction"
author: "Albesa Istrefaj, Antonia Durisch, Philipp Drebes"
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: yes
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to our project on employee turnover prediction! In this project, we will be using various machine learning algorithms to analyze a dataset containing employee turnover data and develop predictive models.

Employee turnover is a critical issue for companies, as it can result in significant costs such as recruitment, training, and lost productivity. Identifying factors that contribute to employee turnover and developing accurate predictive models can help companies better understand employee behavior and improve employee retention.

## Preparation and Exploration

Load the libraries.

```{r}
library(tidyverse, quietly = T)
library(vioplot, quietly = T)
library(caret, quietly = T)
library(vtreat, quietly = T)
library(ggcorrplot, quietly = T)
library(readr, quietly = T)
library(ggplot2, quietly = T)
library(gridExtra, quietly = T)
library(mgcv, quietly = T)
library(e1071, quietly = T)
library(dplyr, quietly = T)
library(mlbench, quietly = T)
library(caret, quietly = T)
library(arm, quietly = T)
```

Load and inspect the data.

```{r}
turnover <- read_csv('data/turnover.csv', col_names=TRUE)
# summary(turnover)

# Having a look at the structure.
str(turnover)
```

## Exploration

```{r}
# scatterplot of stag vs. extraversion
plot.scatter <- ggplot(data = turnover, aes(x = stag, y = extraversion)) +
  geom_point()

# visualizing the distribution of a categorical variable - distribution of industry
plot.distca <- ggplot(data = turnover, aes(x = industry)) +
  geom_bar()

# distribution of a continuous variable - box plot of age by gender
plot.distco <- ggplot(data = turnover, aes(x = gender, y = age)) +
  geom_boxplot()

grid.arrange(plot.scatter, plot.distca, plot.distco, ncol=2)

par(mfrow = c(2,3))

# histogram of the age
hist(turnover$age)

# boxplot of the age
boxplot(turnover$age)

# plot of age and extraversion
plot(turnover$age, turnover$extraversion)

# barplot of gender - male and female
barplot(table(turnover$gender))

# density plots
plot(density(turnover$age))

# violin plot
vioplot(turnover$age)
```

### One-Hot encoding

```{r}
dummy <- dummyVars(" ~ .", data=turnover)

#perform one-hot encoding on data frame
turnover.oh <- data.frame(predict(dummy, newdata=turnover))

str(turnover.oh)
```

## Linear Models

### Naive Approach

In this chapter, we will be exploring Regression, which is a type of machine learning algorithm used for predicting numerical values. We will explain how regression works, how to train a regression model, and how to use it to predict the likelihood of an employee leaving the company.

```{r}
correlationMatrix <- cor(turnover.oh)
ggcorrplot(correlationMatrix, hc.order = TRUE)

# find attributes that are highly corrected ( > 0.75 )
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated <- sort(highlyCorrelated)

# print indexes of highly correlated attributes
print(highlyCorrelated)
names(turnover.oh[highlyCorrelated])

turnover.reduced = turnover.oh[,-c(highlyCorrelated)]

ggcorrplot(cor(turnover.reduced), hc.order = TRUE)
```

```{r}
fit <- lm(event ~ ., data = turnover.reduced)
summary(fit)
```

#### Interpretation

It appears that this model has not a good fit...

### Non-linearities

First we inspect all numerical coefficients for non-linearities.

```{r}

## Age
gg.age <- ggplot(data = turnover, mapping = aes(y = event, x = age)) + 
  geom_point()

plot.age <- gg.age + geom_smooth()

## Extraversion
gg.extraversion <- ggplot(data = turnover, mapping = aes(y = event, x = extraversion)) + 
  geom_point()

plot.extraversion <- gg.extraversion +  geom_smooth()

## Independence
gg.independence <- ggplot(data = turnover, mapping = aes(y = event, x = independ)) + 
  geom_point()

plot.independence <- gg.independence + geom_smooth()

## Self-control
gg.selfcontrol <- ggplot(data = turnover, mapping = aes(y = event, x = selfcontrol)) + 
  geom_point()

plot.selfcontrol <- gg.selfcontrol + geom_smooth()

## Anxiety
gg.anxiety <- ggplot(data = turnover, mapping = aes(y = event, x = anxiety)) + 
  geom_point()

plot.anxiety <- gg.anxiety + geom_smooth()

## Innovator
gg.innovator <- ggplot(data = turnover, mapping = aes(y = event, x = novator)) + 
  geom_point()

plot.innovator <- gg.innovator + geom_smooth()

grid.arrange(plot.age, plot.extraversion, gg.independence, plot.selfcontrol, plot.anxiety, plot.innovator, ncol=2)
```

It appears that only the factor age has a non-linear behavior. We will use a GAM to find its complexity.

```{r}
gam.turnover.1 <- gam(event ~ traffic + way + s(age), 
                      data = turnover, family = binomial)
summary(gam.turnover.1)

plot(gam.turnover.1, residuals = TRUE, select = 1)
```

### Feature Selection

```{r}
# ensure results are repeatable
set.seed(7)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(event~., data=turnover.reduced, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)

# summarize importance
print(importance)
# plot importance
plot(importance)

summary(model)
```

### GLM

```{r}
glm.turnover <- glm(event ~ traffic + way + age + profession, 
                    data = turnover, family = binomial)
summary(glm.turnover)

plot(glm.turnover)
```

## Support Vector Machines

In this chapter, we will be exploring the Support Vector Machines (SVM) algorithm. SVM is a powerful algorithm for classification and regression, and is commonly used in machine learning. We will explain how SVM works, how to train an SVM model, and how to use it to predict employee turnover.

```{r}
set.seed(123)
indices <- createDataPartition(turnover.reduced$event, p=.85, list = F)

train <- turnover.reduced %>%
  slice(indices[, 1])
test_in <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  dplyr::select(-event)
test_truth <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  pull(event)
```

## Train the Neural Network

```{r}
# Fit the SVM model on the training data
svm_model <- svm(event ~ ., data = train, kernel = "linear",
                 type = "C-classification", scale = TRUE, cost = 100)


# Make predictions on the testing data
test_pred <- predict(svm_model, newdata = test_in)

# Convert test_truth to a factor with the same levels as test_pred
test_truth <- factor(test_truth, levels = levels(test_pred))

# Evaluate the performance of the model using confusion matrix
conf_matrix <- confusionMatrix(test_pred, test_truth)
conf_matrix
```

Conclusion

## Artificial Neural Networks

In this chapter, we will be exploring Artificial Neural Networks (ANN). ANN is a machine learning algorithm inspired by the biological structure of the human brain. We will explain how ANN works, how to train an ANN model, and how to use it to predict employee turnover.

```{r}

```
