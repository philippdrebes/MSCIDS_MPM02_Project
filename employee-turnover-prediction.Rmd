---
title: "Employee Turnover Prediction"
author: "Albesa Istrefaj, Antonia Durisch, Philipp Drebes"
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: yes
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to our project on employee turnover prediction! In this project, we will be using various machine learning algorithms to analyze a dataset containing employee turnover data and develop predictive models.

Employee turnover is a critical issue for companies, as it can result in significant costs such as recruitment, training, and lost productivity. Identifying factors that contribute to employee turnover and developing accurate predictive models can help companies better understand employee behavior and improve employee retention.

## Preparation and Exploration

Load the libraries.

```{r, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse, quietly = T)
library(vioplot, quietly = T)
library(caret, quietly = T)
library(vtreat, quietly = T)
library(ggcorrplot, quietly = T)
library(readr, quietly = T)
library(ggplot2, quietly = T)
library(gridExtra, quietly = T)
library(mgcv, quietly = T)
library(e1071, quietly = T)
library(dplyr, quietly = T)
library(mlbench, quietly = T)
library(caret, quietly = T)
library(arm, quietly = T)
library(doParallel, quietly = T)
```

Load and inspect the data.

```{r}
turnover <- read_csv('data/turnover.csv', col_names=TRUE)
# summary(turnover)

# Having a look at the structure.
str(turnover)
```

## Exploration

```{r}
# scatterplot of stag vs. extraversion
plot.scatter <- ggplot(data = turnover, aes(x = stag, y = extraversion)) +
  geom_point()

# visualizing the distribution of a categorical variable - distribution of industry
plot.distca <- ggplot(data = turnover, aes(x = industry)) +
  geom_bar()

# distribution of a continuous variable - box plot of age by gender
plot.distco <- ggplot(data = turnover, aes(x = gender, y = age)) +
  geom_boxplot()

grid.arrange(plot.scatter, plot.distca, plot.distco, ncol=2)

par(mfrow = c(2,3))

# histogram of the age
hist(turnover$age)

# boxplot of the age
boxplot(turnover$age)

# plot of age and extraversion
plot(turnover$age, turnover$extraversion)

# barplot of gender - male and female
barplot(table(turnover$gender))

# density plots
plot(density(turnover$age))

# violin plot
vioplot(turnover$age)
```

### One-Hot encoding

```{r}
dummy <- dummyVars(" ~ .", data=turnover)

#perform one-hot encoding on data frame
turnover.oh <- data.frame(predict(dummy, newdata=turnover))

str(turnover.oh)
```

## Linear Models

In this chapter, we will be exploring linear models and try to use them to predict the likelihood of an employee leaving the company.

First we start by removing highly correlated variables from our data set, as they might have a negative influence on our models.

```{r}
correlationMatrix <- cor(turnover.oh)
ggcorrplot(correlationMatrix, hc.order = TRUE)

# find attributes that are highly corrected ( > 0.75 )
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
highlyCorrelated <- sort(highlyCorrelated)

# print indexes of highly correlated attributes
print(highlyCorrelated)
names(turnover.oh[highlyCorrelated])

turnover.reduced = turnover.oh[,-c(highlyCorrelated)]

ggcorrplot(cor(turnover.reduced), hc.order = TRUE)
```

```{r}
fit <- lm(event ~ ., data = turnover.reduced)
summary(fit)
```

#### Interpretation

It appears that this simple linear model has not a good fit. Only 14% of the variation is explained by the model.

### Non-linearities

First we inspect all numerical coefficients for non-linearities.

```{r}

## Age
gg.age <- ggplot(data = turnover, mapping = aes(y = event, x = age)) + 
  geom_point()

plot.age <- gg.age + geom_smooth()

## Extraversion
gg.extraversion <- ggplot(data = turnover, mapping = aes(y = event, x = extraversion)) + 
  geom_point()

plot.extraversion <- gg.extraversion +  geom_smooth()

## Independence
gg.independence <- ggplot(data = turnover, mapping = aes(y = event, x = independ)) + 
  geom_point()

plot.independence <- gg.independence + geom_smooth()

## Self-control
gg.selfcontrol <- ggplot(data = turnover, mapping = aes(y = event, x = selfcontrol)) + 
  geom_point()

plot.selfcontrol <- gg.selfcontrol + geom_smooth()

## Anxiety
gg.anxiety <- ggplot(data = turnover, mapping = aes(y = event, x = anxiety)) + 
  geom_point()

plot.anxiety <- gg.anxiety + geom_smooth()

## Innovator
gg.innovator <- ggplot(data = turnover, mapping = aes(y = event, x = novator)) + 
  geom_point()

plot.innovator <- gg.innovator + geom_smooth()

grid.arrange(plot.age, plot.extraversion, gg.independence, plot.selfcontrol, plot.anxiety, plot.innovator, ncol=2)
```

It appears that only the factor age has a non-linear behavior. We will use a GAM to find its complexity.

#### Training the model

Performing feature selection before training a model can be a good practice to improve model performance and reduce overfitting. Here we use recursive feature elimination (RFE) before training the model. We will again use only the one-hot encoded data set with correlating features included, as we expect the feature selection process to take care of it.

```{r}
# Define the control parameters for feature selection
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

# Specify the formula for the GAM model
formula <- event ~ s(age, bs = "tp", k = -1) + .

# Define a smaller range of subset sizes
subset_sizes <- c(1:10)

# Reduce the number of values in the tuning grid
tune_grid <- data.frame(nselect = 1:10)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# Run the feature selection algorithm
rfe_result <- rfe(turnover.oh[, -2], turnover.oh$event,
                   sizes = subset_sizes, rfeControl = control,
                   method = "gam", tuneGrid = tune_grid, verbose = FALSE)

# Stop the parallel processing
stopCluster(cl)

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

unregister_dopar()

# Get the selected features
selected_features <- names(turnover.reduced[, -2])[rfe_result$optVariables]
print(selected_features)
```

No features where selected. Therefore, we will use again the `turnover.reduced` data set.

We will try a different approach.

```{r}
# ensure results are repeatable
set.seed(12)

turnover.reduced$event = as.factor(turnover.reduced$event)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Enable parallel processing
cl <- makeCluster(8)  # number of cores as per your machine
registerDoParallel(cl)

# train the model
model.gam <- train(event ~ ., data=turnover.oh, 
               method="gam", preProcess=c("scale", "center"), trControl=control)

# Stop the parallel processing
stopCluster(cl)
unregister_dopar()

# estimate variable importance
importance <- varImp(model.gam, scale=FALSE)

# summarize importance
# print(importance)
plot(importance)

summary(model.gam)
plot(model.gam, residuals = TRUE)
```

Still not a good fit, with only around 12% of the variance explained by the model. Also we see, that the feature selection only has a slight, almost negligable, impact on accuracy.

### Generalized Linear Models

```{r}
glm.turnover <- glm(event ~ traffic + way + age + profession, 
                    data = turnover, family = binomial)
summary(glm.turnover)

plot(glm.turnover)
```

## Support Vector Machines

In this chapter, we will be exploring the Support Vector Machines (SVM) algorithm. SVM is a powerful algorithm for classification and regression, and is commonly used in machine learning. We will explain how SVM works, how to train an SVM model, and how to use it to predict employee turnover.

```{r}
set.seed(123)
indices <- createDataPartition(turnover.reduced$event, p=.85, list = F)

train <- turnover.reduced %>%
  slice(indices[, 1])
test_in <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  dplyr::select(-event)
test_truth <- turnover.reduced %>%
  slice(-indices[, 1]) %>%
  pull(event)
```

## Train the Neural Network

```{r}
# Fit the SVM model on the training data
svm_model <- svm(event ~ ., data = train, kernel = "linear",
                 type = "C-classification", scale = TRUE, cost = 100)


# Make predictions on the testing data
test_pred <- predict(svm_model, newdata = test_in)

# Convert test_truth to a factor with the same levels as test_pred
test_truth <- factor(test_truth, levels = levels(test_pred))

# Evaluate the performance of the model using confusion matrix
conf_matrix <- confusionMatrix(test_pred, test_truth)
conf_matrix
```

### Conclusion

## Artificial Neural Networks

In this chapter, we will be exploring Artificial Neural Networks (ANN). ANN is a machine learning algorithm inspired by the biological structure of the human brain. We will explain how ANN works, how to train an ANN model, and how to use it to predict employee turnover.

```{r}

```
